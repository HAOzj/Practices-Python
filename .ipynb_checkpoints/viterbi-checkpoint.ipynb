{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词性标注器**  \n",
    "词性标注器(POS tagger)的作用是给文本的每个单词赋予一个词性(POS, Part Of Speech).词性可以提供关于所在词和上下文的信息.比如动词后一般跟名词.词性信息可以帮助语法分析或者命名实体识别等.  \n",
    "\n",
    "**序列标注**  \n",
    "在这里,文本被看作词的序列,而每个词被赋予词性则是获得标注.所以词性标注器也是序列标注的一个例子.    \n",
    "  \n",
    " \n",
    " \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建模**  \n",
    "\n",
    "所谓[建模](https://en.wikipedia.org/wiki/Mathematical_model)就是用数学的语言来描述一个系统,用来解释系统运行机制,分析各种因素的影响以及做预测.  \n",
    "\n",
    "**词性标注建模**  \n",
    "\n",
    "给定一个文本,也就是一个词序列$\\{w_i\\}_{i=1}^l$.  \n",
    "我们求  \n",
    "  $ argmax _{ \\{tag_i\\}_{i=i}^l }   P( \\{tag_i\\}_{i=i}^l | \\{w_i\\}_{i=1}^l)$  \n",
    "也就是该组词下,出现概率最大的一组POS tags.  \n",
    "\n",
    "根据贝叶斯法则, $ P( \\{tag_i\\} | \\{w_i\\} ) =  \\frac {P( \\{w_i\\}|\\{tag_i\\} ) *   P( \\{tag_i\\})} {P(\\{w_i\\})}    $.  \n",
    "\n",
    "这样的话,词性标注问题就变成了一个概率问题.而分母${P(\\{w_i\\})} $实际上扮演了归一化函数的角色,也就是使得所有可能的词性标注序列的概率和为1.  \n",
    "\n",
    "为了简化问题，我们扔掉分母,最大化$P( \\{w_i\\}|\\{tag_i\\} ) *   P( \\{tag_i\\})$.因为即使没有归一化,我们仍然可以通过比较来得出最大化分子的词性标注序列$\\{tag_i\\}_{i=i}^l$.而这个序列才是我们真正关心的. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HMM(Hidden Markov Model)隐马模型**  \n",
    "\n",
    "HMM对上述问题进行了两个方面的简化,  \n",
    "1. tags序列的一阶markov性,也就是第i个tag的概率只依赖于第i-1个tag.那么$P{\\{tag_i\\}_{i=i}^l} =  \\prod_{i=1}^{l} P(tag_i|tag_{i-1})$  \n",
    "2. 每个$w_i$的概率只依赖于该词的tag,和周围的词无关.也就是$P(\\{w_i\\}_{i=1}^l) = \\prod_{i=1}^{l}P(w_i|tag_i)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P( \\{w_i\\}_{i=1}^l|\\{tag_i\\}_{i=i}^l ) *   P( \\{tag_i\\}_{i=i}^l) =  \\prod_{i=1}^{l} P(tag_i|tag_{i-1}) P(w_i|tag_i) $  \n",
    "1. $P( w_i|tag_i) $叫做emission(溢出概率),表示给定一种POS tag生成一个词的概率;\n",
    "2. $P( tag_i|tag_{i-1})$ 叫做transition(转移概率)，表示给定前一个词的词性,后一个词词性的概率分布 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**动态优化**  \n",
    "\n",
    "[动态规划](https://en.wikipedia.org/wiki/Dynamic_programming)是用来解决具有最优子结构(大规模问题的最优解由形式完全一样的小规模问题的最优解组成)的问题.相比于简单的归纳法,动态规划储存了小规模问题的最优解,避免了解决大规模问题时反复计算同一小规模问题.  \n",
    "在词性标注中,第1个词到第i个词的词性标注就是规模为i的问题,该问题最优解就是    \n",
    "$ argmax _{ \\{tag_i\\}_{i=i}^j }   P( \\{tag_i\\}_{i=i}^j | \\{w_i\\}_{i=1}^j)$ \n",
    "\n",
    "**Viterbi算法--求隐马模型的动态规划方法**  \n",
    "\n",
    "隐马模型赋予每个词性标注序列一个\"概率\"(加双引号是因为我们去掉了归一化函数,并不是真正意义的概率).基于这个概率,我们需要求出概率最大的词性标注序列.  \n",
    "\n",
    "用运筹学的语言来表述,每个词性标注序列都是一种可能的解,每个解都对应一个数值(这里是\"概率\").我们需要求出最大化该数值的解.  \n",
    "[Viterbi算法](https://en.wikipedia.org/wiki/Viterbi_algorithm)由Andrew Viterbi在1967年提出,用动态规划的方法来解决这个优化问题.我们可以借助小规模问题的最优解来获得大规模问题的最优解(我们最关心的规模为l的问题的最优解).  \n",
    "\n",
    "我们用一个表(也就是dynamic programming中programming的最初意义)来记录从第1个词开始,每种词性标注序列的概率.如\n",
    "\n",
    "\n",
    "| 词性 | 开头 | I | love | dogs | 结束|\n",
    "| -| - | :-:  | -: | - | - |\n",
    "| 开头 | 1 | 0    | 0  | 0 | 0| \n",
    "|  人称代词 | 0 | 0.16 | 0  | 0.013464| 0 |\n",
    "| 动词 | 0 | 0.09 | 0.0748 | 0 | 0 |\n",
    "| 名语 | 0 | 0 | 0 | 0.04488 | 0 |\n",
    "|结束| 0 | 0 |0 | 0| 0.1017952|\n",
    "\n",
    "\n",
    "第i行j列的节点记录了第j个词为i词性时的最优词性标注路径(包含了j词前面词的词性序列)上上一个词的词性,黑体字表示了句子的最优词性标注序列\n",
    "\n",
    "| 词性 | 开头 | I | love | dogs | 结束|\n",
    "| -   | - | :-:  | -: | -  | |\n",
    "| 开头 |  |     |   |  | | \n",
    "|  人称代词| | **0** |  |    2||\n",
    "| 动词 |  | 0 | **1** |  |    |\n",
    "| 名语 |  |  |  | **2** |    |\n",
    "|结束 |  |    |   |  |  3|\n",
    "\n",
    "0:开头  1:人称代词 2:动词 3:名词 4:结束\n",
    "\n",
    "所以end节点就储存了所有词(所有词都在end词前)的最优词性标注路径.这里最优指的是隐马模型下词性标注序列的概率.  \n",
    "\n",
    "我的代码实现中用两个矩阵,命名为Viterbi和path,来分别记录概率(隐马模型下词性标注序列的概率)和最优解(词性标注序列)\n",
    "\n",
    "Viterbi(i, j)记录了限定第j个词为i词性,规模为j的词性标注问题的最优解对应的概率.代表第j个词为i词性时,前j个词的词性标注序列的最优解的值.通过迭代来实现:  \n",
    "\n",
    "$Viterbi(i,j) =  max_{k=1}^{K} emission(j, i)* Viterbi(k,j-1)* transition[k,j]$  \n",
    "\n",
    "用path(i,j)来记录第j个词为i词性时,前j个词的词性标注序列的最优解中第j-1个词的词性.一步步回溯,我们就可以获得这个最优解.    \n",
    "\n",
    "为了从后向前获得整个句子的最优词性标注序列,  \n",
    "1. t=l,$tag_l^{*} = argmax_{i} Viterbi(i,t)$,也就是从第1个词到最后1个词,概率最大时最后一个词的POS tag.$tag_l^{*}$就是最后一个词的POS tag,    \n",
    "2. $tag_{t-1}^{*} = path(tag_{t+1}^{*},t+1)$.从最后一个词的POS tag来追溯使得它概率最大的倒数第二个词的POS tag.$tag_{t-1}^{*}$就是倒数第二个词的POS tag\n",
    "3. t = t-1，  \n",
    "3. 重复2,3步,直到得到所有词的POS tag  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi算法实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词为e, 词性为e, 概率为1.7952000000000004%\n",
      "词为dogs, 词性为n, 概率为4.488%\n",
      "词为love, 词性为v, 概率为7.48%\n",
      "词为I, 词性为rr, 概率为16.000000000000004%\n",
      "词为b, 词性为b, 概率为100.0%\n",
      "[[ 1.        0.        0.        0.        0.      ]\n",
      " [ 0.        0.16      0.        0.013464  0.      ]\n",
      " [ 0.        0.09      0.0748    0.        0.      ]\n",
      " [ 0.        0.        0.        0.04488   0.      ]\n",
      " [ 0.        0.        0.        0.        0.017952]]\n",
      "[[ 0.  4.  4.  4.  4.]\n",
      " [ 0.  0.  4.  2.  4.]\n",
      " [ 0.  0.  1.  4.  4.]\n",
      " [ 0.  4.  4.  2.  4.]\n",
      " [ 0.  4.  4.  4.  3.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on the 13th June 2018\n",
    "\n",
    "@author : woshihaozhaojun@sina.com\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def viterbi_algo(text, transition, emission, ind2tag, word2ind ):\n",
    "    '''\n",
    "    pos tagger的viterbi算法\n",
    "    \n",
    "    Args:\n",
    "        text(iterables)      :- 要估计的句子的序列,长度为l\n",
    "        transition(np.array) :- 词性转换矩阵， \n",
    "                                [i,j]元素表示从i词性到j词性的概率，\n",
    "                                维度为[K ,K], K为词性的种类数\n",
    "        emission(np.array)   :- 产生词的概率矩阵,\n",
    "                                [i,k]元素表示i词性生成k词的概率,\n",
    "                                维度为[K, v], v为字典的大小\n",
    "        ind2tag(iterables)   :- 第i个元素为i词性\n",
    "        word2ind(dict)       :- k词为key，序号为value\n",
    "    Returns:\n",
    "        paths(np.array)      :- [i,w]元素表示第w词为i词性时上一个词的词性，\n",
    "                                维度为[n, l]\n",
    "        viterbi(np.array)    :- [i,w]元素表示第w词为i词性的概率,\n",
    "                                维度为[K,l]\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        assert transition.shape[0] == transition.shape[1]\n",
    "    except AssertionError:\n",
    "        print(\"转移矩阵不是方阵\")\n",
    "        \n",
    "    try:\n",
    "        assert transition.shape[0] == emission.shape[0]\n",
    "    except AssertionError:\n",
    "        print(\"emission矩阵的行数和词性数不一致\")\n",
    "        \n",
    "    try:\n",
    "        assert len(ind2tag)==transition.shape[0]\n",
    "    except AssertionError:\n",
    "        print(\"ind2tag长度和词性数不一致\")\n",
    "        \n",
    "    cols = len(text) \n",
    "    rows = transition.shape[0]\n",
    "\n",
    "    paths = np.zeros(( rows, cols))\n",
    "    viterbi = np.zeros((rows, cols))\n",
    "    viterbi[0,0] = 1\n",
    "    for j in range(1,cols):\n",
    "        for i in range(rows):\n",
    "            prob = viterbi[:,j-1] *  transition[:,i]* emission[i, word2ind[ text[j]] ] # [cols, 1]\n",
    "            sort = np.argsort(prob)\n",
    "            paths[i,j] =  sort[-1]\n",
    "            viterbi[i,j] = max(prob)\n",
    "            \n",
    "    last =  int(np.argsort(  viterbi[:, j] )[-1]) # 最后一个词的概率最大的行序\n",
    "\n",
    "    print(\n",
    "        \"词为{}, 词性为{}, 概率为{}%\".format( text[-1], ind2tag[last], viterbi[last, j]*100) \n",
    "    )\n",
    "    for j in range(cols-1):\n",
    "        last = int(paths[ last, cols -1 -j    ]) # 上一个词的行序\n",
    "        \n",
    "        print( \n",
    "            \"词为{}, 词性为{}, 概率为{}%\".format( text[-2-j], ind2tag[last],  viterbi[ last  ,cols -2 - j]*100)\n",
    "        )\n",
    "\n",
    "    return paths, viterbi\n",
    "\n",
    "def demo():\n",
    "    text = ['b', 'I', 'love', 'dogs', 'e'] \n",
    "\n",
    "    ind2tag = ['b', 'rr', 'v','n', 'e'] # 开头，人称代词，动词，名词\n",
    "\n",
    "    transition = np.array( \n",
    "        [\n",
    "            [0, 0.4, 0.2,  0.4, 0], # 从b到 rr, verb, noun, e\n",
    "            [0, 0,   0.85, 0.1, 0.05],\n",
    "            [0, 0.3, 0,    0.6, 0.1],\n",
    "            [0, 0, 0.6,  0, 0.4],\n",
    "            [0, 0, 0, 0, 0]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    emission = np.array(\n",
    "        [\n",
    "            [1, 0 , 0, 0, 0],\n",
    "            [0, 0.4, 0, 0.6, 0],\n",
    "            [0, 0.45 , 0.55 , 0, 0],\n",
    "            [0, 0 , 0 , 1, 0],\n",
    "            [0, 0 , 0 , 0 , 1]\n",
    "        ]\n",
    "\n",
    "    )\n",
    "\n",
    "    word2ind = {\n",
    "        'b' : 0,\n",
    "        'I' : 1,\n",
    "        'love' :2,\n",
    "        'dogs' : 3,\n",
    "        'e' :4\n",
    "    }\n",
    "    paths, viterbi = viterbi_algo(text, transition, emission, ind2tag,word2ind)\n",
    "    print(viterbi)\n",
    "    print(paths)\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
